/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
   Copyright 2021 The CFU PLayground Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/lite/kernels/internal/reference/integer_ops/mnv2_conv.h"

#include "mnv2_cfu.h"
#include "perf.h"
#include "tf_util/print_params.h"

#ifdef SHOW_CONV_PERF
#define PERF_START(n) perf_enable_counter(n)
#define PERF_END(n) perf_disable_counter(n)
#else
#define PERF_START(n)
#define PERF_END(n)
#endif

//
// This file contains specialized conv 2D implementations to support
// MobileNet v2 models
//
namespace tflite {
namespace reference_integer_ops {

inline static void LoadOutputChannelWeights(const int32_t*& output_multiplier,
                                            const int32_t*& output_shift,
                                            const int32_t*& bias_data,
                                            int batch_size) {
  PERF_START(3);
  CFU_SET_OUTPUT_BATCH_SIZE(batch_size);
  for (int i = 0; i < batch_size; i += 4) {
    CFU_STORE_OUTPUT_MULTIPLIER(*(output_multiplier++));
    CFU_STORE_OUTPUT_MULTIPLIER(*(output_multiplier++));
    CFU_STORE_OUTPUT_MULTIPLIER(*(output_multiplier++));
    CFU_STORE_OUTPUT_MULTIPLIER(*(output_multiplier++));
  }
  for (int i = 0; i < batch_size; i += 4) {
    CFU_STORE_OUTPUT_SHIFT(*(output_shift++));
    CFU_STORE_OUTPUT_SHIFT(*(output_shift++));
    CFU_STORE_OUTPUT_SHIFT(*(output_shift++));
    CFU_STORE_OUTPUT_SHIFT(*(output_shift++));
  }
  for (int i = 0; i < batch_size; i += 4) {
    CFU_STORE_OUTPUT_BIAS(*(bias_data++));
    CFU_STORE_OUTPUT_BIAS(*(bias_data++));
    CFU_STORE_OUTPUT_BIAS(*(bias_data++));
    CFU_STORE_OUTPUT_BIAS(*(bias_data++));
  }
  PERF_END(3);
}

inline static void LoadFilterValues(const uint32_t*& filter_words,
                                    int num_words) {
  PERF_START(4);
  for (int i = 0; i < num_words; i += 8) {
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
    CFU_STORE_FILTER_VALUE(*(filter_words++));
  }
  PERF_END(4);
}

inline static void LoadInputValues(const uint32_t*& input_ptr,
                                   int input_depth_words) {
  PERF_START(6);
  for (int i = 0; i < input_depth_words; i += 2) {
    CFU_STORE_INPUT_VALUE(*(input_ptr++));
    CFU_STORE_INPUT_VALUE(*(input_ptr++));
  }
  PERF_END(6);
}

inline static void UnloadOutputValues(uint32_t*& output_ptr, int num_words) {
  PERF_START(7);
  // This works
  if (num_words & 2) {
    *(output_ptr++) = CFU_GET_OUTPUT();
    *(output_ptr++) = CFU_GET_OUTPUT();
  }
  for (; num_words > 2; num_words -= 4) {
    *(output_ptr++) = CFU_GET_OUTPUT();
    *(output_ptr++) = CFU_GET_OUTPUT();
    *(output_ptr++) = CFU_GET_OUTPUT();
    *(output_ptr++) = CFU_GET_OUTPUT();
  }

  // // This works
  // for (int i = num_words; i > 0; i -= 2) {
  //   *(output_ptr++) = CFU_GET_OUTPUT();
  //   *(output_ptr++) = CFU_GET_OUTPUT();
  // }

  // Original
  // for (int i = 0; i < num_words; i++) {
  //   *(output_ptr++) = CFU_GET_OUTPUT();
  // }
  PERF_END(7);
}

// Fixed-point per-channel-quantization convolution reference kernel.
void Mnv2ConvPerChannel1x1(
    const ConvParams& params, const int32_t* output_multiplier,
    const int32_t* output_shift, const RuntimeShape& input_shape,
    const int8_t* input_data, const RuntimeShape& filter_shape,
    const int8_t* filter_data, const RuntimeShape& bias_shape,
    const int32_t* bias_data, const RuntimeShape& output_shape,
    int8_t* output_data) {
  PERF_START(2);
  // Get parameters.
  const int32_t input_offset = params.input_offset;  // r = s(q - Z)
  const int32_t output_offset = params.output_offset;

  // Set min and max value of the output.
  const int32_t output_activation_min = params.quantized_activation_min;
  const int32_t output_activation_max = params.quantized_activation_max;

  // Consistency check
  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);
  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);
  const int input_depth = MatchingDim(input_shape, 3, filter_shape, 3);
  const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
  if (bias_data) {
    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);
  }
  // Check dimensions of the tensors.
  const int output_height = output_shape.Dims(1);
  const int output_width = output_shape.Dims(2);

  // Set parameters for op
  const int input_depth_words = input_depth / 4;
  CFU_SET_INPUT_DEPTH_WORDS(input_depth_words);
  CFU_SET_OUTPUT_DEPTH(output_depth);
  CFU_SET_INPUT_OFFSET(input_offset);
  CFU_SET_OUTPUT_OFFSET(output_offset);
  CFU_SET_ACTIVATION_MIN(output_activation_min);
  CFU_SET_ACTIVATION_MAX(output_activation_max);

  // Access filter data as words
  const uint32_t* filter_words = (const uint32_t*)filter_data;

// Do the processing in batches, by output channel. batch size is number of
// output channels processed per batch and it is chosen to avoid overflowing
// filter_data memory, and then rounded down to a multiple of 4.
//
// For each batch, the entire input will be read once
#ifdef USE_CONV_SMALL_BATCHES
  const int channels_per_batch =
      std::min(output_depth, (2048 / input_depth) / 4 * 4);
#else
  const int channels_per_batch =
      std::min(output_depth, (NUM_FILTER_DATA_BYTES / input_depth) / 4 * 4);
#endif

  const int num_pixels = output_height * output_width;
  const int num_batches =
      (channels_per_batch - 1 + output_depth) / channels_per_batch;
  PERF_END(2);

  for (int batch = 0; batch < num_batches; batch++) {
    const int batch_base = batch * channels_per_batch;
    const int batch_end =
        std::min(output_depth, batch_base + channels_per_batch);
    const int batch_size = batch_end - batch_base;

    // Load up output channel parameters and filter values
    LoadOutputChannelWeights(output_multiplier, output_shift, bias_data,
                             batch_size);
    LoadFilterValues(filter_words, batch_size * input_depth_words);

    PERF_START(5);
    // Reset input and output pointers
    const uint32_t* input_ptr = (uint32_t*)input_data;
    uint32_t* output_ptr = (uint32_t*)(output_data + batch_base);

    for (int p = 0; p < num_pixels; p++) {
      // Load twice on first loop, no load on last loop and once every other
      // time.
      if (p == 0) {
        LoadInputValues(input_ptr, input_depth_words);
      }
      if (p != num_pixels - 1) {
        LoadInputValues(input_ptr, input_depth_words);
      }

      CFU_MACC_RUN();
      UnloadOutputValues(output_ptr, batch_size / 4);
      output_ptr += (output_depth - batch_size) / 4;
    }
    PERF_END(5);
  }
}

}  // namespace reference_integer_ops
}  // namespace tflite
